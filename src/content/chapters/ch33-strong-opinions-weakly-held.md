# Chapter 33: Strong opinions. Weakly held

_Part 3_

Last Friday my mate sent me a screenshot at 08:41. It was an AI “market scan” for his pitch, six competitors, three prices each, and a neat little conclusion at the bottom saying who was “winning” and why. He’d circled one line in red like it was the key. It looked deadly. Then you read it and realise there isn’t a single source in it, not one link, not one date, not even a “this might be wrong.”


This is the new danger zone for marketing teams. Not bad output. Polished output that floats.
AI is useful, obviously. I use it constantly. But it has a habit of giving you the thing you wanted in the format you wanted, even when it has no right to be that sure. It sounds calm, it sounds confident, it sounds like it’s already been through review. That tone is doing a lot of heavy lifting.


Before, rough work looked rough. A messy doc made you think. A half-baked argument made you ask questions. Now you can get a clean page in seconds and the clean page seduces you into skipping the awkward bit, which is checking what’s actually true.
Most marketing claims aren’t lies. They’re just unverified. We’ve always lived with that. “Best practice” floats around. Case studies get treated like laws. Dashboards show a number with two decimals and everyone nods because it’s easier than reopening the whole debate. AI doesn’t create that weakness, it just scales it. It gives you more claims per minute than any team can properly interrogate.


Narayanan and Kapoor have a line that nails why this stuff lands so easily. “Broken AI is very appealing to broken institutions.” It’s not a tech insult. It’s a human one. If the organisation is under pressure, under-skilled, under-resourced, or just allergic to uncertainty, then anything that offers certainty, even fake certainty, becomes attractive.


That’s where the “skeptically optimistic” behaviour matters. It’s not about being the person who rolls their eyes at AI. That’s just cynicism dressed up as maturity. It’s about staying open to what the tools can do, while insisting on evidence before you let the output steer a real decision.


The fastest way to explain the difference is this. A cynic dismisses. A sceptic checks.


The checking sounds boring until you look at what happens when people don’t do it. Michigan’s MiDAS unemployment system is one of those examples you almost wish wasn’t real, because it’s too clean as a warning. It cost $47 million. It achieved a 93% error rate. It wrongly accused roughly 40,000 people of fraud between 2013 and 2015, collected $21 million wrongfully, and eleven thousand families filed for bankruptcy.


There’s a lot in that in a few numbers. A high error rate is bad, obviously. The more interesting part is how an institution reacts to a system that gives confident outputs at speed. If the output arrives like a verdict, people start treating it like a verdict. The system becomes the grown-up in the room. It allows everyone else to stop thinking as hard, because “the system flagged it.”


That line, “the computer says,” shows up in other stories too. Robert Williams in Detroit, arrested in front of his daughters in January 2020 because a facial recognition system tagged him. He was the ninth-best match. When he protested, the officer said, “The computer says it’s you.” Detroit settled for $300,000 in July 2024.


Same shape. A confident output takes weight it hasn’t earned, because it’s easier than admitting we’re guessing.


Marketing isn’t putting people in jail, thankfully. But we do the same thing with softer consequences. We let a model output become authority because it lets us move on.
Attribution is the obvious one. A platform tells you what “drove” the sale, a dashboard colours in the journey like a primary school workbook, and everyone nods because the alternative is a longer conversation about uncertainty. AI makes that habit worse because it can generate the story around the number as well. Not just “paid search drove this,” but a full paragraph explaining why, plus three recommendations, plus a plan for next quarter. It’s a whole little universe, spun out of thin air, in perfect grammar.


This is why I keep coming back to a simple line. Confidence is not correlated with accuracy. Not for humans, and not for AI. AI just delivers the confidence more consistently than we do.


So what does skeptical optimism actually look like day to day, when you’re tired and busy and the work keeps coming.


First, you get strict about what counts as a fact. When the model states something as true, you either verify it or you label it as a guess. No third option. If you can’t check it, you treat it like a hypothesis, not an input.
That one habit saves you from most of the expensive mistakes, because most expensive mistakes are not dramatic hallucinations. They’re small untruths that slide into a plan because they sound plausible. A feature that used to exist. A price that changed. A regulation that got updated. A competitor move that happened in the US but not here. Boring errors, cleanly written.


Second, you force definitions. AI loves vague nouns because vague nouns let it sound right in more situations. “Effectiveness.” “Engagement.” “Brand love.” “Value prop.” It will happily blend them into one soup and your brain will happily accept the soup because it tastes familiar.


You have to pin it down. When you say “effectiveness,” do you mean sales this week, profit this quarter, mental availability next year, lower churn, higher price tolerance. Because those are different jobs. If you don’t specify, you end up with recommendations that sound sensible and solve nothing.


Third, you look for the trade-off. Real strategy hurts a little. There’s always a sacrifice. You don’t get everything. You can’t be for everyone. You can’t win on every dimension.


AI outputs often read like a dream where nobody has to choose. Increase reach, improve conversion, strengthen brand, lower costs, and keep everyone happy internally. Grand. If a plan has no downside, it’s usually because it skipped the hard bit and wrote the ending anyway.
Fourth, you make it argue with itself. This is the simplest way I’ve found to stop yourself using AI as a validation machine.
Ask it to make the case for the idea, then make the case against it, then ask what evidence would decide between the two. Not “what feels right,” but what you would actually need to see. Research. Behaviour. A test result. A sales pattern. A customer quote. Something that exists outside the chat window.


When it can’t name deciding evidence, that’s useful. It’s showing you it’s guessing. When it can, it gives you a checklist that makes your thinking sharper.
There’s a related point from the Cursor team that I keep thinking about. They call it the “verification problem.” The model can generate more output than a human can check. Output becomes cheap. Verification becomes the bottleneck.


That’s a big deal for marketing teams because we were already drowning in output. Decks, docs, dashboards, meeting notes, endless “quick thoughts.” AI pours petrol on the pile. If you don’t build checking into the workflow, you end up in the MiDAS trap, different stakes, same pattern. Automated certainty, human passivity.


Aman Sanger from Cursor says these models are strong reflections of the data they were trained on, and when you push into areas that don’t exist much online, the brittleness shows. That’s basically marketing in a sentence. If you’re asking about generic topics with lots of public examples, AI will look brilliant. If you’re asking about your exact category, your exact customers, your legal constraints, your distribution quirks, your weird brand history, it can still look brilliant, and be wrong in a way you won’t notice until you ship.


So another habit is to test brittleness on purpose. Ask the same question twice, slightly differently. Push an edge case. Ask it what assumptions it is making. Ask what would have to be true for the opposite answer to be correct.


Humans show uncertainty through wobble. Models smooth uncertainty into neatness. That smoothness is not proof of truth. Sometimes it’s the opposite.
Now, none of this is a reason to become gloomy. Cynicism is lazy. It’s the same as gullibility, just with a different haircut.


Generative AI is genuinely useful. It can draft, compress, rephrase, summarise, generate options, and act as a sparring partner. It can help you get unstuck. It can do the boring admin that makes people quietly hate their jobs. You’d be mad not to use it.


But you have to decide what role it plays. If you treat it like an oracle, you’ll get comfortable lies, delivered politely. If you treat it like an intern, bright, eager, fast, and occasionally full of it, you’ll get something better. Drafts, options, pressure tests. Then you do the grown-up bit, which is judgement.
This links back to something I wrote years ago about decision-making. We don’t change our minds easily. Consistency and commitment kicks in. We tell the agency we like the script, and suddenly we need to keep liking it. We sell it internally and now we’re trapped by our own sales job. Pain and effort makes it worse. If we’ve sweated for months, our brain doesn’t want to admit it might be wrong.


AI adds a new layer to that. Because now you can outsource the first draft and it comes back so polished you feel committed to it faster. You didn’t sweat for months, but you still feel a weird loyalty because it looks “done.” The pain has been replaced by speed, and speed creates its own kind of commitment. The doc exists, so it becomes real.


That’s why the “strong opinions, weakly held” idea matters even more now. Not as a slogan. As a practical defence against tidy nonsense.


One technique I still use is thinking in probabilities, not certainties. If you’re about to make a call, ask yourself, what’s the chance this works, and why. Say it out loud. “I think this is a 60% bet because X and Y, and it fails if Z.” It deflates the drama. It makes room for someone to disagree without it turning into a personality clash. It also makes it easier to change your mind later, because you didn’t pretend you were certain in the first place.
You can do the same with AI output. Treat it like a bet, not a verdict.


And here’s the optimistic twist. AI can help you be more sceptical, if you set it up that way.
Have it generate counter-arguments. Have it list the assumptions it made. Have it point out where it’s guessing. Have it act like a hard client and ask you the questions you’re trying to avoid. Have it rewrite your strategy in plain language, then see if you still believe it when the fancy words are stripped out.


The job isn’t to collect answers. The job is to work out which answers don’t stand up. That’s the behaviour. Assume useful, then prove. The risk is that teams fall in love with speed and start mistaking output for progress. The opportunity is that the teams who keep their standards will move faster without drifting into fog.


Back to my mate’s screenshot. I asked him one question, where did the prices come from. He replied with a shrug emoji, which is not a source, even in 2026. We laughed, then he went back and found the actual pages. Two of the numbers were wrong. One competitor had changed pricing the month before.
Nothing dramatic happened. No scandal. No red faces. Just a small avoided mistake.


That’s what this behaviour looks like most of the time. Not heroics. Little pauses. Little checks. A refusal to let a clean sentence bully you into believing it. And you won’t do it perfectly. You’ll still ship things that turn out wrong. We all do.


But you improve your odds when you keep your scepticism healthy and your optimism intact, and you don’t let a tool that sounds confident take your judgement off you.




















Build
