# Chapter 32: Interview with two dead men

_Part 3_

Last Tuesday I was on the sofa with the laptop half open, Slack pinging, a crumpled Tesco meal deal bag on the armrest, and a deck on screen with a tidy little chart and two decimal places. Someone had written “+12.47%” like it had been carved into stone. Nobody asked where it came from. It looked deadly.

Most marketing claims aren’t lies. They’re just unverified. And we treat them like they’ve been through a proper trial because the slide looks clean and the numbers have decimals.

You see it every day with attribution. The model tells you paid search “drove” the sale, the dashboard highlights the path in blue, and everyone nods because it looks serious and we all have other things to do. The work isn’t checking, the work is keeping up.

Case studies do the same trick. They tell you what worked, you forward them on, and after a while they turn into proof. Never mind that they’re written to impress, lit like a film set, then reused in boardrooms as if context is a small detail you can ignore.

“Best practice” is the final boss. Say something often enough and it stops sounding like a suggestion. Eventually nobody remembers who tested it, when they tested it, what counted as success, or whether the world looked anything like yours.

Foxes have always been sceptical, not cynical. Cynics dismiss everything and call it wisdom. Sceptics stay open to the idea that something could be true and helpful. They just want it to be earned. What’s changed is that now the unverified stuff sounds better than ever.

I read something Benedict Evans wrote that resonated. Conference organisers used AI to write his biography. It read perfectly. Right kind of education. Right kind of career. Right tone. Every single detail wrong, not wild mistakes, just adjacent ones. For the organisers, who couldn’t verify, it passed. For Evans, who could scan and fix it in seconds, it was usable.

I had my own version of the same thing when I ran Claude’s Deep Research on my first book, purely to live-demo it for students. The output was properly impressive. It found themes I’d half forgotten. It pulled quotes. It spotted links across chapters. It even structured the whole thing better than I probably would have on a tired day.

Then it slid in one small hallucination. It said I’d interviewed David Ogilvy and Claude Hopkins for the book.

Ogilvy died in 1999. Hopkins in 1932.

One mistake, but it’s the kind that wrecks you if you miss it. You’d be standing there, calm as you like, citing chats with two dead men like it’s a normal Tuesday.

That’s the real problem with AI and evidence. Not that it gets things wrong. Everything gets things wrong. It’s that the wrong bits sit right beside the right bits, delivered with the exact same confidence. When most of it is sharp, we stop checking. We skim, we trust, then we repeat a claim that never happened with a straight face.

Evans circles the same point from another angle when he says he doesn’t have many use cases where “roughly right” is helpful. That line matters more than it sounds. Marketing has always tolerated approximation, close enough strategy, directionally correct insights, a decent proxy dressed up as causation. AI didn’t start that habit, but it makes the habit feel safer because the output is tidy and the tone is sure of itself.

Ethan Mollick calls AI “an omniscient, eager-to-please intern who sometimes lies to you.” It predicts what words come next, not what’s true. When confident nonsense is plausible, confident nonsense is what you get. And it lands smoothly because it sounds like the sort of thing that would be true.
People underestimate the risk because they picture a daft mistake and a red face. The failure mode is usually cost.

I’ve seen teams take AI-generated competitor analysis into pricing conversations, confident because it cited sources, listed features, and looked exhaustive. Nobody checked whether those features were current. They weren’t. A year-old pricing page had been scraped, summarised, and laundered into “market reality.” The outcome wasn’t embarrassment. It was a six-figure pricing decision made on information that no longer existed.

No hallucinated dead men. Just stale facts, neatly wrapped.

So looking for evidence still starts the same way, but the checks shift a bit now. We still go to the original source, but with AI we also ask where the model could have filled gaps. If something sounds oddly generic, we assume it might be inferred rather than observed. We ask which parts came from documents and which parts came from pattern-matching.

We still check definitions, but we now watch for silent blending. AI loves collapsing distinctions. “Effectiveness” turns into a soup of awareness, engagement, and sales unless you force it to separate them. We still ask what was left out, but now we’re alert to false completeness. Long answers feel thorough. 

They’re often just verbose. The question becomes, what would have been hard for this system to know? And we add one new habit. We test brittleness on purpose. Ask the same question twice, slightly differently. Push an edge case. See what changes. Humans contradict themselves when they’re unsure. Models smooth over uncertainty. That smoothness is the tell. This isn’t about being awkward for sport, it’s about putting back the pause that AI quietly removes.
Gordon Pennycook found that the strongest predictor of who falls for impressive-sounding nonsense isn’t intelligence or education. It’s whether they pause. People who stop and think, even briefly, are far less likely to be fooled. Annoying. Also hopeful. That pause is learnable. It’s a behaviour.

Carl Bergstrom and Jevin West built an entire course around this kind of thinking because most people were never taught how to question confident claims properly. AI didn’t create that hole. It just widened it, faster output, cleaner language, fewer reasons to stop.

The fox questions still work. Find the source. Check what the words actually mean. Ask what else could explain the result. Look for the missing context. Stress-test the certainty. Not because we’re cynical, but because we’ve all seen how easy it is to be confidently wrong, especially now that the machines are very good at sounding sure.
